[{"categories":null,"content":"个人介绍 我的昵称是nigo，一条逆行的狗。 毕业于国防科学技术大学，注册会计师，边防连队任职步兵排长 3 年，复员后从事财务审计 2 年，此后从事 IT 审计至今。 个人利用擅长 python 、sql、 clickhouse 等技术进行数据分析。 爱好折腾 Linux 、emacs。 ","date":"2023-02-11","objectID":"/about/:1:0","tags":null,"title":"个人介绍","uri":"/about/"},{"categories":null,"content":"微信公众号 2018年 4 月开通微信公众号：逆行的狗，记录平时审计工作中所思、所学、所想。 最开始写过一堆用于审计的 VBA 工具，认识了一些志同道合的朋友。 平时发文较多的主要是在微信公众号上。 ","date":"2023-02-11","objectID":"/about/:2:0","tags":null,"title":"个人介绍","uri":"/about/"},{"categories":null,"content":"BiliBili 在 B 站的账号是nigo81，有时会录制一些关于 python 、sql、 Linux 相关的视频。 ","date":"2023-02-11","objectID":"/about/:3:0","tags":null,"title":"个人介绍","uri":"/about/"},{"categories":null,"content":"出版物 在工作期间在电子工业出版社出版过两本书，每本书都在 B 站上传了对应的配套视频。 《审计效率手册》 《 IT 审计：用SQL+Python提升工作效率》 虽然自己不是什么专家，但平时工作中比较喜欢学习和思考，所以写作对自己来说也就是学习和思考的输出，这对我是非常有益的。 ","date":"2023-02-11","objectID":"/about/:4:0","tags":null,"title":"个人介绍","uri":"/about/"},{"categories":["效率"],"content":"作为重度知识使用者，我使用过很多笔记软件。 包括为知笔记、印象笔记、 gingko 、vimwiki。 其中停留在 vimwiki 的时间比较长，它是编辑器之神 vim 的一个插件，用它积累了很多学习、工作的笔记，全部是本地的 makrdown 文件，共享盘同步。 从去年 8 月开始，用上了 emacs ，这个被称为神之编辑器的东西，确实让人上瘾，几乎可以用它来做任何事情。 这个编辑器不仅属于程序员，其实很多国外学术圈的人也很多人使用。 目前我使用它记录笔记、工作安排、 GTD 、写 Python 、写公众号、听收音机、发邮件、读 RSS 。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:0:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"读书笔记 在看书时，可以方便地一边记笔记、一边看 PDF ，可以全部使用键盘。 使用dictionary-overlay插件，可以在阅读英文时，遇到不懂的单词可以直接标注，后面再次出现也会显示中文。 使用popweb插件可以直接实时查有道词典。 搭配着这两个插件，基本上我也不怕读英文文档了。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:1:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"org-mode emacs最强大的就是 org-mode ，可以方便组织文字、图片、链接 每一层级可以方便的折叠，展开，非常有逻辑性。 同时，对于学习代码来说，他还可以直接运行，因此我们可以进行文学编程。 只需要在代码块中按下C-c C-c就可以将代码结果运行出来。 如果你喜欢双链笔记，可以使用org-roam插件。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:2:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"编写代码 搭配上lsp-bridge插件，可以瞬间变成一个成熟的 IDE ，尤其是如果平时要写多个语言，那么在一个编辑中写书，更高效。 毕竟快捷键都是一套自己熟悉的。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:3:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"GTD org-agenda 是非常强大的任务管理插件。 在工作时，如果有一项任务来临，按下C-c c就可以弹出记录。 输入待办事项、开始时间或结束时间、重要程度 也可以使用番茄钟等对任务的时间消耗进行记录。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:4:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"RSS 可以订阅自己喜欢的一些博客文章或者播客、 B 站关注的 UP 主。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:5:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"邮件 最近也把邮件客户端省了，打开 emacs 就可以查阅和发送邮件。 对联系人也是可以直接补全的。 当收到邮件时，还可以和前面org-agenda结合将邮件做一项待办任务，后面再进行处理。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:6:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["效率"],"content":"结语 emacs是一个存在了 40 多年的自由软件，到现在也还具有强大的生命力，可以预见未来的 40 年也将继续存在。 它是神之编辑器，在 windows 、Linux、 Mac 上都可以使用。 如果感兴趣可以看下陈斌写的《一年成为 Emacs 高手 (像神一样使用编辑器)》 https://github.com/redguardtoo/mastering-emacs-in-one-year-guide/blob/master/guide-zh.org 刚开始上手配置比较困难，可以直接使用他的配置文件： https://github.com/redguardtoo/emacs.d 它的配置文件应该是支持 windows 、mac、 linux 的。 不过后面自己熟悉后，还是可以折腾一份自己的配置，毕竟自己能掌控的才是最好的。 ","date":"2023-02-10","objectID":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/:7:0","tags":["emacs"],"title":"让人上瘾的emacs","uri":"/posts/20230205232142-%E8%AE%A9%E4%BA%BA%E4%B8%8A%E7%98%BE%E7%9A%84emacs/"},{"categories":["工作"],"content":"在实践中， IT 审计团队通常是和财务审计团队配合完成审计工作的，区别是财务审计是对被审计单位的财务报表及其附注发表意见，而 IT 审计是对信息系统发表意见。 我们所做的工作是通过测试财务报表所依赖的信息系统(包括财务系统和业务系统）的有效性、数据的真实性，证明信息系统环境是否可以信赖。从某种意义上来说， IT 审计是服务于财务审计。 随着财务审计对 IT 审计工作的了解，他们会对 IT 审计提出更多 IT 审计工作范围外的期待。 尤其是在企业信息化浪潮下，受限于技术能力，财务审计团队在数据处理，方法创新方面对 IT 审计团队提出了更高的要求。因此 IT 审计如何更好地辅助财务审计是一个新的课题。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:0:0","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"利用Python代替繁重计算 在财务审计中存在大量数据处理、数据计算的基础性工作，对于这类有逻辑重复性工作， IT 审计团队可以借助 Python 批量完成。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:1:0","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"成本还原 对于生产制造业的 IPO 审计项目，成产品的料工费占比是财务审计特别关注的事项。而很多企业采用了逐步结转分步法来核算生产成本，由于每一道工序的半成品将是下一道工序的原材料，所以要计算产成品真实的料工费占比就需要进行成本还原。 如果企业使用的 ERP 系统没有还原后的成本结构报表，同时财务成本会计也未手工编制成本结构报表，那么这项核查工作对于财务审计来说将是巨大的工作量。 对于这类财审提出的需求，我们会了解企业的生产工艺，以及成本还原的计算方法，再利用 Python 编程语言，模拟成本还原的计算过程，将产成品拆分成真实的料工费，从而计算出料工费的占比。 通过 IT 审计团队的辅助，财审团队能完成人工短时间无法完成的工作。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:1:1","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"保费收入与收款流水核对 在一家保险公司 IT 审计项目中，财审团队需要我们对保费收入与收款流水进行数据核对。 当我们了解到业务中存在大量多个保单对应多个收款流水的情况，对于这种多对多关系，正常是无法批量核对的。 一般情况下，我们核对的数据关系要么是一对一、多对一或者一对多。这三种情况我们写 SQL 语句时都很好处理，只需要将多条数据按单号聚合汇总再进行核对就可以。 这种多对多的关系我们可以借助数学图论中的二分图解决。 比如，我们将这种关系画上线，可以看到其中可以形成一些独立的网络，这里我们将这种网络（用不同颜色标记）简称为组。 我们将独立的网络编上组号，然后将两个数据集的金额分别按组号汇总再进行核对。 示例 Python 代码： class Net(object): def __init__(self): self.setA = { 'A': ['a', 'b', 'c'], 'B': ['d'], 'C': ['b'], 'D': ['a', 'c'], 'E': ['e'] } self.setB = { 'a': ['A', 'D'], 'b': ['A', 'C'], 'c': ['A', 'D'], 'd': ['B'], 'e': ['E'], } self.keys = list(self.setA.keys()) + list(self.setB.keys()) self.groups = {} def split_net(self): num = 0 for key in self.keys: if key not in self.groups.keys(): num += 1 self.loop_net(key, num) def loop_net(self, node, id): if node: self.groups[node] = id if node in self.setA.keys(): sub_nodes = self.setA[node] else: sub_nodes = self.setB[node] for sub_node in sub_nodes: if sub_node: if sub_node not in self.groups.keys(): self.loop_net(sub_node, id) if __name__ == '__main__': net = Net() net.split_net() print(net.groups) 我们借助 Python 构建了类Net,通过 loop_net函数递归找出两个数据集形成的独立网络，从而给独立网络分组，最后我们可以将两个数据集分别按组号聚合后进行核对。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:1:2","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"利用Python获取外部信息 在财务审计过程中，外部信息的可靠性大于内部信息，因此审计师会查询大量的外部信息与获取的内部信息做交叉验证。 例如，审计一家汽车硬件销售的企业，财务审计团队计划查询销售订单中对应的汽车车架号的信息，核实有没有真实车辆以及车型信息，以及判断查询的外部信息与订单信息是否存在矛盾。 企业 1 年的销售订单有 30 万，依赖审计师人工查询将不太现实。那么，我们 IT 审计团队提出了两种方案： 购买接口，通过 python 调用接口批量获取数据。 编写爬虫，通过 python 编写爬虫获取公开网站数据。 出于节约成本考虑我们选择编写爬虫获取公开信息，用 python 的selenium包，自动查询网站数据，并将结果保存到本地文件。 视频 最后再由财务审计团队对外部信息与内部信息的一致性进行判断。 当然，对于很多公开网站都会有反爬措施，如验证码、访问频率限制、字体混淆等，对于我们 IT 审计团队提出了一定的技术要求。 除此外，财务审计对于外部信息的获取有很大的需求，如同行业财务指标、公司公告、发函快递信息、汇率、交易性金融资产价格等等。 这类需求，随着这几年的发展已有很多成熟的商业网站可以满足，但对于一些特殊的、行业属性较强的小众信息，如果不能批量查询，那么 IT 审计团队就可以辅助财务审计完成信息获取工作。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:2:0","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"利用IT技术进行方法创新 2020年 6 月24日，证监会依法对獐子岛公司信息披露违法违规案作出行政处罚及市场禁入决定。证监会将渔船的北斗定位信息，通过第三方机构还原出航行轨迹，从而计算出采捕区域面积，进而估算真实成本。 根据这则公开披露的信息，启发了我们利用信息技术的优势，对相关数据进行深入分析挖掘，使审计工作更加智慧、高效。我们 IT 审计团队相对有信息技术的优势，而财务审计团队在财务处理、项目风险识别等方面更有优势，两个团队的良好融合会发挥1+1\u003e2的效果。 对于这类精纬度空间信息，我们可以通过 Python 计算面积、周长、距离。 示例 Python 代码： from pyproj import Geod # 导入Geod类 from shapely.geometry import Point, LineString, Polygon # 导入点、线、多边形类 # 计算封闭区域面积、周长 geod = Geod(ellps=\"WGS84\") # 创建一个WGS84坐标系 polygon = Polygon([(116.169465, 39.932670), (116.160260, 39.924492), (116.150625, 39.710019), (116.183198, 39.709920), (116.226950, 39.777616), (116.442621, 39.799892), (116.463478, 39.790066), (116.588276, 39.809551), (116.536091, 39.808859), (116.573856, 39.839643), (116.706380, 39.916740), (116.600293, 39.937770), (116.514805, 39.982375), (116.499935, 40.013710), (116.546520, 40.030443), (116.687668, 40.129961), (116.539697, 40.080659), (116.503390, 40.058474), (116.468800, 40.052578)]) # 将多个精纬度坐标实例化为Polygon多边形对象 poly_area, poly_perimeter = geod.geometry_area_perimeter(polygon) # 计算多边形面积和周长 print(poly_area, poly_perimeter) # 打印面积和周长 面积:951546279.1726327 周长:183419.43445625657 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:3:0","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["工作"],"content":"结语 随着新技术、新模式在企业发展过程中不断涌现，财务审计与 IT 审计的融合将会更加紧密。在这样的背景下， IT 审计的工作界限未来可能会更加模糊， IT 审计如何发挥我们的优势辅助财务审计高效、智慧地完成审计工作需要我们共同探索。 ","date":"2022-09-18","objectID":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/:4:0","tags":["IT审计","python"],"title":"IT审计之辅助财务审计","uri":"/posts/20220918151947-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E8%BE%85%E5%8A%A9%E8%B4%A2%E5%8A%A1%E5%AE%A1%E8%AE%A1/"},{"categories":["生活"],"content":"逐梦 壹 梦，是模糊的、虚幻的 你看不清它的样子 只能跟着它的影子 一路狂奔 ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:1:0","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"楔子 傍晚，答完最后的考题，提前在电脑上点击了确认提交的按钮。 我兴奋地走出注册会计师的考场， 长舒一口气，终于结束了两天 CPA 的6门考试。 门口熙熙攘攘的人群，我像小学生一样蹦蹦跳跳地跑向在门口等我的大学同学龙哥。 他和他媳妇凤姐等着为我庆祝重要的考试。 成都，满是自由的空气，我感觉到前所未有的轻松。 “考得怎么样？”龙哥问。 “应该还可以。” 简单几句话后，龙哥开着车带着我去撸串了。 就着酒，向龙哥讲述着我这几年的辛苦生活， 感受着这自由的一切，满怀对未来的憧憬。 ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:1:1","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"缘起 对于我来讲，梦的起点，即是CPA 也是整篇故事的开头。 它成了我对未来所有希望的载体， 虽然，考它的原因只是因为这是我唯二能报名的考试， 但，它就是波涛汹涌的大海中的一片绿叶， 托着我脑海中整个海市蜃楼。 回忆过去的一年， 随着军号， 6 点半起床， 每天看书到晚上 12 点。 偶尔也去菜地，锄锄地， 偶尔也去跑个五公里，出出汗。 CPA就是我的全部， 是我改变命运的机会， 是向反对我的父母的证明， 我相信在哪里我都可以混得很好， 我可以吃苦，我也拥有长期训练出来的“智力”， 而我需要的只是一个机会， 以及给我证明的时间。 和龙哥干完最后一杯酒， 我仿佛看到几年前我们一起训练、一起学习的情景， 入肚的酒气和回忆， 化为内心无尽的信心和勇气。 我来了， 属于我的自由， 属于我的未来！ ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:1:2","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"逐梦 贰 走，干审计！ 一次高中同学聚会，一位女同学在信永中和会计师事务所成都分所工作，我给她说我也去考了 CPA ，已经过了三门了，打算以后去事务所工作，你们平时工作怎么样？ 她大概给我讲了些，说平时工作比较辛苦，出差比较多。 我想这辛苦能有多辛苦，总比白加黑，5+2好吧。 “事务所待遇怎么样？”我问。 “她说，你想挣多少？”，她说。 “干个两三年，有 5000 块不？”我小心翼翼地问。 她笑了笑，“那肯定是有的。” “好，那我就去事务所试试。”,心里暗自下了决定。 ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:2:0","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"碰壁 2016年 4 月，我就回到了家。 向同学咨询了下，他们要年底才招人。 我想了想，反正还有剩下三门考试要考，就等到下半年去应聘，正好也可以准备下考试。 虽然媳妇没有上班，涵涵才 1 岁，肚子里也有了悦悦， 但这段时间单位还是继续发工资，所以也没有什么压力，自己每天就陪陪家人，复习考试。 大概到了 9 月份，一边准备着考试，一边开始投简历了。 为了保险起见，我百度了下会计师事务所的排名，分别到他们的官网都或者邮箱都投递了简历。 但都石沉大海。 当时我投递的简历就长这样： 我也实在找不到与工作职位相符的经历， 我总不可能写熟练操作山地步兵武器： 95 式自动步枪， 120 火箭筒， 82 毫米无后坐力炮， 100 毫米迫击炮，高射机枪…….吧。 别人肯定会想，“哥，你是来干审计的，还是来砸场子的？” 连写的“单位相关财务工作”也只是和我们司务长经常聊天，硬扯的。 虽然写的熟练运用 office 办公软件，其实连 Excel 都还没有用过。 等了几天，实在等不下去了，怎么办？ 只能发扬“没有条件也要创造条件”的优良传统了。 总体策略就是线下+线上的模式。 线下 我拉出四川排名前 10 的事务所办公地址，跑到成都待了两天，直接到公司前台去投简历。 什么立信、大华、华信全跑了，一般前台收了简历就告诉我后面有招聘的时候会通知我，然后就没有然后了。 只有一个前台老师，年纪比较大了，忘记哪个所了，让我等一下，很负责任地跑去他们领导那里问了下，然后告诉我最近他们不招人。 一次、两次，信心还是有所动摇了，不知道自己找不找得到工作。 线上 由于我认识的人也只有一个前面说的高中同学在事务所，也没有谁能帮忙介绍下。 我就跑到信永的贴吧去，看有没有了解的人指条路： 当时翻贴吧，看到最多的就是这位“不倒的斯嘉莉”，给大家回复招聘的邮箱，我一直在猜想应该是位 HR ，也不知道我猜得对不对。 当然，这也没有什么用，但好在看到同样一批求职的人在问，心里稍微感觉好点。 然后我又加了很多 QQ 群，看看有没有朋友能帮忙介绍的。 当时有个深圳的以前天职的大哥,很热心，帮我介绍给了成都这边他认识的领导。 屁颠屁颠地还跑去参加了天职在某个高校的校招的宣讲会，让我到年底的时候等他们通知。 当时很感谢这位大哥，他后面回成都了，约了几次吃饭，都没约成功，没有当面感谢他。 总之，这段时间都没有任何结果，只能老老实实看书了。 ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:2:1","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"柳暗花明 大概是考完注会后，终于收到信永的笔试通知了。 当时高兴坏了，总算看到了一丝希望。 接下来，很顺利地通过笔试。 后面是面试， 还记得是人力蒋总、汪经理、夏经理面试我， 开始让我自我介绍，简短介绍了下后， 就是问专业问题， 汪经理问：“审计货币资金要执行哪些审计程序。” 当时我飞速思考了三秒，最多就是三秒。 我发现我都忘记完了，因为会计这门是去年考过的。 我赶紧回答，“不知道。” 这时，汪经理笑了笑说：“哦，你应该是忘记了，后面真正工作了和实务结合起来就印象深刻了。” 结果，居然就没有再问我专业问题了，反而对我之前的工作比较感兴趣。 然后，就开始聊之前的生活，说着说着三位领导都不停在笑。 整个面试就在轻松的氛围下结束了。 出来，我还问了一起面试的朋友，他们被不停地问各种专业问题，我感觉自己运气太好了。 接着就是合伙人面试，当时是大罗总面试的我。 走进他办公室，他摘下眼镜，看着我的简历，非常儒雅的气质。 “我们这刚开始工资比较低，你的期望工资是多少？”罗总问。 “ 2000 ”，我说。 他突然忍不住地笑了笑说：“ 2000 倒不至于。” 不过，当时我就是想学东西，只要能管吃住，不给钱都愿意干。 罗总说：“给你两年的时间，一年 20 万不成问题。” 虽然后面自己干得并不好，但当时他的话让我对未来还是充满了期望。 走出他办公室，感觉稳了，心里憧憬着能成为一名注册会计师，能有一技之长，能混口饭吃。 大概 1 个月后，我收到了录取通知，凭着自己努力终于找到自己想要的工作了。 走，干审计！ ","date":"2022-08-27","objectID":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/:2:2","tags":["逆行的狗"],"title":"逐梦 壹","uri":"/posts/20220827231902-%E9%80%90%E6%A2%A6_%E5%A3%B9/"},{"categories":["生活"],"content":"下完班，一行四人坐地铁回酒店。 其实酒店并不远，坐一站地铁就可以到，但地铁里走的时间远远超过了坐地铁的时间。 今天的广州下着小雨，出站后我们在人行道等着红绿灯。 突然，我看着小林带着一个外国小哥走到梅总的旁边， 小哥让梅总帮忙接听下电话，给滴滴司机说下他在哪里。 开始我还以为小哥说的粤语，听不懂。 梅总帮他把订单取消了，然后给他指了路。 全程，我就听到梅总最后说了个Of course 中间我问小林，为什么你自己不给小哥说，要让梅总去？ 小林说：“我英语不好。” 我说：“你这个英语是可以的，指个路没问题。” 她说：“我是有家室的人。” 我看了看小哥， 180 几，高高瘦瘦，一幅清秀的样子。 “你怕是想得有点多哦，别人只是问个路，你还来个I hava family” 翻了一个大大的白眼给她。 ","date":"2022-08-25","objectID":"/posts/20220825232249-%E7%BB%99%E8%80%81%E5%A4%96%E6%8C%87%E8%B7%AF/:0:0","tags":["杂文"],"title":"给老外指路","uri":"/posts/20220825232249-%E7%BB%99%E8%80%81%E5%A4%96%E6%8C%87%E8%B7%AF/"},{"categories":["生活"],"content":"时间一天一天过去， 它的离去甚至都不曾通知你， 挤在早高峰的地铁， 拘谨的空间中也无人认识你， 每天做着一件又一件重复的事情， 迎着朝霞，送走夕阳， 走过无数个城市， 当夜晚来临， 蜗居在廉价酒店， 吃着一份外卖， 这又是未曾通知且不曾期待的一天。 ","date":"2022-08-22","objectID":"/posts/20220822234714-%E6%97%A0%E9%A2%98_2022_08_22/:0:0","tags":["杂文"],"title":"无题 2022-08-22","uri":"/posts/20220822234714-%E6%97%A0%E9%A2%98_2022_08_22/"},{"categories":["工作"],"content":"“数据是 21 世纪的石油，而分析则是内燃机。” 如果说数据是石油，其本身是无价值的，只有对数据深度挖掘，才能为企业业务增长提供新的引擎，形成真正的数据资产。 近年来，随着信息技术飞速的发展，企业信息化建设已由类似于 ERP 等行业属性相对较强的信息系统建设转向信息系统之上的数据管理与业务应用建设，如建设适应企业业务发展的数据中台、业务中台等新型 IT 架构。通过建设敏捷高效可复用的支撑平台，为业务数字化创新提供高效数据和服务支撑。 IT审计执行的信息系统一般控制、应用控制测试是对企业信息科技领域管控的评价，仍然是对信息系统基础设施及建立其上的业务流程的控制测试。面对企业业务开展所产生的海量数据，其勾勒出了企业经营活动真实画像，数据核查工作在应对舞弊、异常检测方面显得越来越重要。 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:0:0","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"数据核查的特点 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:1:0","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"由抽样审计转变为全量审计 IT审计的数据核查大多时候是为财务审计服务的。受限于技术手段，财务审计在进行数据核查时，往往是抽样审计。但面对类似电商、游戏等这样的互联网企业所产生的海量销售订单，抽样检查多少个合适呢？ 100 个、 1000 个还是 10000 个才合适呢？ 借助于 SQL 、ClickHouse等大数据分析技术，我们可以对上亿行的数据量进行全量核查，全量分析。 例如，对于大型集团企业，审计在执行银行流水与财务序时账核对时一般仅对大额流水进行检查，并且会耗用大量人力和时间。如果企业开通了银企直联，银行流水与序时账一般会有关联字段，我们可以利用 SQL 进行全量双向核对；如果企业未开通银企直联，或没有关联字段，我们仍然可以利用 Python 按照人工核对的逻辑编写代码，实现网银流水与序时账的全量核对，不再区分金额大小。 再如，对于生产工艺复杂的制造型企业，其工序可能多达十几步或者几十步，审计难以对生产成本进行重新计算以验证存货计量的准确性。但对于计算机来说，这些工序的成本分摊逻辑是一致的，借助于 Python ，我们也是可以实现对所有工单的生产成本的归集和分摊进行全量重新计算。对于这些收入、成本计算逻辑复杂的企业，利用 Python 这样的编程语言，复现系统的计算逻辑，能取得很好的数据核查效果。 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:1:1","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"数据分析的颗粒度更小 财务审计在执行分析性程序时，往往使用的数据颗粒度很大，如按年或月的汇总金额去进行波动分析。数据的颗粒度就像一张照片的像素，当颗粒度很大时，照片所呈现的信息将会失真，很多细节信息将难以发现。而 IT 审计进行数据核查时一般按照最小颗粒度的数据进行多维度分析，如订单、小时、分钟、渠道等，这是异常检测的基础。 如上图所示，当我们将订单按照一天 24 小时划分为 24 个区间，分别统计每个小时区间的订单金额时，能发现 2019 年0点和 2020 年8点的订单金额显示异常。这就是将分析的颗粒度变小的好处，能还原更多细节信息。 如上图所示，我们甚至可以按分钟去统计次数，去检测是否存在利用机器人等技术短时间大量刷单的情况。 如上图所示，我们可以以订单的颗粒度去分析单价的稳定性，对于发散的或者偏离正态分布的数据检测出来，进一步去核查异常数据产生的原因。 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:1:2","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"数据核查对象多样化 在大数据时代，几乎所有的人、事、物都能够数据化，进而被分析。 我们将数据核查的对象可以划分为结构化数据和非结构化数据，结构化数据即为能够用数据或者统一结构加以表示的信息，如信息系统中的各种报表。而非结构化数据，就是一些无法用数字或统一的结构表示，如合同、发票、邮件、网页等。 IT审计数据核查的对象不再局限于财务账、业务报表等结构化数据，借助于新的 IT 技术，我们可以将数据核查的范围延伸到非结构化的数据。 如上图所示，审计一家航运企业，以前我们只能通过手工抽样查询船舶定位位置与业务系统中的班期表核对，验证航行的真实性。现在我们可以利用 Python 爬虫技术，批量解析网页中船舶经纬度信息和出发地、目的地，全量核对。 如上图所示，借助于 OCR 技术，我们可以将非结构化的发票图片文件识别成结构化数据，从而实现发票的全量核查。 当然，我们还可以将数据划分为财务数据、业务数据、日志数据三种类型，财务审计在做数据核查时更多核查的是财务数据。而 IT 审计核查的对象会延伸到业务数据和日志数据。 从企业舞弊造假成本来说，=财务数据\u003c业务数据\u003c日志数据=，我们更倾向于通过日志数据、业务数据的核查来验证财务数据的真实性、准确性、完整性。 如上图所示，审计一家制造型企业，我们获取了公司 ERP 系统的操作日志，按天对作业频率进行分析，检查作业频率异常偏高的情况，以排查是否存在舞弊迹象。由于操作日志真实反映了人员的所有系统操作，其数据的可信度相比财务数据更高，能更好地应对舞弊欺诈行为。 总之， IT 审计的数据核查的来源具有多样化的特点。 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:1:3","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"数据核查的方法 IT审计数据核查需要将 IT 技术与审计方法相结合。在信息技术飞速发展的浪潮下，我们需要拥抱新技术、新思想、新变革，同时对我们的审计思路、审计模式进行创新。 审计一家游戏企业，玩家通过充值获得游戏币，游戏币可以在商城中购买游戏道具，购买的道具可以自己使用也可以赠送他人。在审计过程中，我们发现一些账号的充值金额异常大，我们利用 Neo4j 图数据库将所有道具的赠送关系进行网络分析： 我们发现消费金额前 25 名的异常账号，其中就有 19 个账号相互之间有赠送行为，形成了网络。通过进一步审计程序，我们发现其中一些账号是淘宝店家，他们通过从价格更低的渠道充值后，以\"赠送\"的方式卖给游戏玩家，因此其充值金额较大，且存在大量赠送行为。 利用 Neo4j 、Gephi、 NetworkX 等工具，我们可以轻松地进行复杂网络关系分析，挖掘出数据背后的关联关系，这是新技术为我们数据核查带来了新的手段、新的方法。 但 IT 技术仅仅是一种工具，我们在做数据核查时，更多的需要和我们的审计方法论结合、和生活常识结合、和行业经验结合、和统计学知识结合。 例如， IPO 的电商企业的数据核查要求我们 IT 审计对是否存在刷单行为进行分析，从数据分析的工具上讲，我们使用 SQL 就能进行分析，但我们从什么维度去分析能发现是否存在刷单行为呢？ 其实我觉得更好的方法是从生活常识入手，从行业经验入手，我们可以去找参与过刷单朋友，询问他们是如何刷单的，了解其特点，然后再设计数据分析的维度。 例如，我向朋友了解到有的刷单方法是找普通的人去购买，发货的时候只发一个空盒子或者价值较小的重量较轻的东西，完成订单后，再通过微信红包的方式返钱给他。 针对这种刷单方式，我们就可以利用“发货重量轻“的特征去筛选出这些异常订单。我们可以根据商品 SKU 的重量信息，计算出系统里一个订单的重量，再去和物流公司发货时称重重量核对，从而检测出重量偏离较大的异常订单。 当然，这只是一个举例，通过这个例子，我想说明在 IT 审计数据核查过程中分析的思路很多时候比技术手段更加重要。 我们正处在信息爆炸、技术变革的时代，我们应该学习新的 IT 技术，积极探索新的审计方法、审计思路，通过深入挖掘数据背后的价值，提高我们的审计质量、审计效率。 ","date":"2022-08-14","objectID":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/:2:0","tags":["IT审计"],"title":"IT审计之独立数据核查","uri":"/posts/20220814214453-it%E5%AE%A1%E8%AE%A1%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%95%B0%E6%8D%AE%E6%A0%B8%E6%9F%A5/"},{"categories":["工作"],"content":"下午被安排给新员工培训 SQL ，带着大家从安装数据库到一些简单的 SQL 语句，讲了 3 个小时。 其实如果大家平时项目上遇不到这种大数据量的情景的话，基本上没有太大的用。但整个分所还是有不少的这样的项目。 唯一的作用感觉就是让他们有一个亲身处理 400 多万行数据的感觉，将来如果遇到了知道学习什么。 其实做一项系统工程，需要处理好三个维度：时间维、逻辑维、知识维。 其中知识维就是要么你知道用什么知识，要么你能用拥有这些知识的人。无论打工者还是老板概莫能外。 对于学习知识的我们来说，其实知识本身并不重要，而习得知识的能力很重要。 你如果留心周围的人会发现，一般厉害的人不只是一方面厉害，更多情况下多方面厉害。一个掌握学习能力的人你随便丢他到陌生的领域，大概率他也会很厉害。 大部分人都停留在表面，觉得做一份工作，学一门知识，自己就是专家了，忽视了背后的基础技能的重要性。其实呢，无论什么工作都有可能被淘汰，任何一门知识都都会变得无用。你平时所做的努力，都在积累背后的某一种能力。 而最能让你有碗饭吃就是这些基础能力。 就像我的领导，我觉得她有一种能力就是说服人的能力，同样一句话，从她嘴里说出来，就能让人信服。有这样的能力，去哪个公司不能当个中层领导？ 就像几年前我加入过\"千熊会员\"，一个四大出来做知识付费的审计师，我发现他有个能力，就是能把一个方法、流程包装成非常吸引人的\"产品\"，靠着这个\"包装\"和总结的能力确实能把知识卖出去。 而我评估自己的能力就是数理逻辑能力，只要是有一定门槛，依赖数理逻辑的专业，我有信心能快速上手。 人的延展性是非常强的，不只一种可能。如果有一天这个行业都没了，总得吃饭吧，我可能为了生计就得去跑外卖或者开滴滴。 如果我是去跑外卖，在熟悉情况后，我也一定会去计算和统计哪种路径、时间、方式去跑能时效比最高。 如果我是去跑滴滴，在熟悉情况后，我也一定会去计算哪个时间段、哪个区域去跑最挣钱。 而我还是得用我最擅长的技能。我还会利用以前在部队宣传股学到的技能，去做自媒体，讲我们外卖小哥的故事。 上高中时，非常喜欢看的美剧《 Heros 》(超能英雄)，我最喜欢的一个角色就是 Peter ，他的超能力就是可以学习其他有超能力的人的超能力，简直太 Bug 了。 还有一个反派角色是 Sylar ，他的超能力是洞悉事物的本质，但是需要挖开别人的脑子，过于残忍，不太喜欢。 最后，想说什么呢？ 多学点别人带不走的东西，时刻保持在哪里都可以吃饭的能力。 ","date":"2022-08-14","objectID":"/posts/20220814214721-%E8%B6%85%E8%83%BD%E8%8B%B1%E9%9B%84/:0:0","tags":["感悟"],"title":"超能英雄","uri":"/posts/20220814214721-%E8%B6%85%E8%83%BD%E8%8B%B1%E9%9B%84/"},{"categories":["工作"],"content":"前几天一个网友发了一个传销组织的数据，他想求每个层级的人数。 Figure 1: 图1 这两天正好学了networkx，我们来看如果用网络分析解决这个问题。 Figure 2: 图2 这是整个推荐关系的可视化网络图。其中正红色的点为根节点。 下面我们一步一步来解决。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:0:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"读取数据并创建网络 我们使用pandas读取 excel 数据，并用nx.from_pandas_edgelist(df,source,target,edge_attr,create_using)函数来创建一个图=G=。 这个函数是根据边数据来创建图，其中： source:df中表示边起始的列名（推荐人）。 target:df中表示边目标的列名（被推荐人）。 edge\\_attr:df中表示边属性的列名（如权重，颜色，大小等）。 create\\_using:表示创建什么类型的图，无向图，有向图等。这里我们使用有向图=DiGraph=,因为推荐关系是有方向的。 import networkx as nx import pandas as pd # 读取数据创建图 df = pd.read_excel('~/传销原始数据.xlsx') df = df.loc[:, ['推荐人ID', '被推荐人ID']] df.columns = ['source', 'target'] G = nx.from_pandas_edgelist(df, 'source', 'target', create_using=nx.DiGraph()) 当然这样创建的图=G=只是一个类的实例化，并不一张真正可视化的图。如果你想可视化它，可以使用=pyvis=包进行，它可以生成一个可交互的网络图。 from pyvis.network import Network nt = Network('650px', '1250px', directed=True) nt.from_nx(G) nt.show('test.html') 这将会生成如图 2 所示的网络图。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:1:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"找到根节点 考虑到这个网络是一个传销组织，那么正常情况下应该是有个唯一的根节点，整个组织类似树状结构。 我们先得找到这个根节点，怎么找呢？ 这就需要先引一个图论中的概念度，度的意思就是一个节点的相邻节点的数量。 Figure 3: 图3 如图 3 所示，如果不考虑边的方向，那点节点 1 有4个相邻节点（有边相连），那么节点 1 的度就是 4 。 即degree=4。 但是这是一个有向图，就会分成in_degree和 out_degree 两种度。 那么我们要找到根结点，只需要去找in_degree==0的节点就是根节点，同理out_degree==0的节点为末级节点。 因此，我们写代码： top_nodes = [n for n, d in G.in_degree() if d == 0] print('root node:', top_nodes) 可以计算出根结点为： [0] Figure 4: 图4 如图 4 所示，我们可以找到图中的根节点，它就是这个网络的头目。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:2:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"计算网络层级关系 为了计算网络层级关系，这里我们需要引入一个概念*距离*，也就是两个节点之间的最短路径长度。 Figure 5: 图5 对于节点 1 到节点 4 的距离为 3 ，因为两条路径可以从节点 1 到达节点 3 ： [1, 2, 3, 4] [1, 2, 5, 4] 这两条路径最短的距离就是 3 。 在networkx库中有个函数nx.shortest_path_length(G,source,target)可以求出节点source和 target 之间的距离。 如果省略target参数，就可以求出source下所有节点与source之间的距离。 因此，我们只需要用=nx.shortest_path_length(G, 0)=就可以求出=根节点0=下的所有节点的距离，也就是*网络层级*。 level = nx.shortest_path_length(G, 0) nx.set_node_attributes(G, level, 'level') level 的值是下面这样的=节点:距离=的字典，可以看到一共 32 个层级。 {0:0,2576:1,..., 5659: 32} 我们求出了所有子节点到根节点的距离level列表，用set_node_attributes()函数给每个节点添加一个层级属性。 下面，我们只需要将level列表，统计出 1-32 层级中分别有哪些节点即可。 # 显示层级 data = {} for n, l in level.items(): if l in data.keys(): nodes = data[l] nodes.append(n) data[l] = nodes else: data[l] = [n] # 打印前10层节点 for l, n in data.items(): if l \u003c 10: print(l, n) 这里我们展示前 10 层级对应的哪些节点： 当然，我们将上面print(l,n)替换成print(l,len(n))，就可以看到每一层级对应的节点数量。 Figure 6: 图6 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:3:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"下线前10的节点 我们知道=度=表示了相邻节点数量，那么度值最大的 10 个，也就是下线数最大的 10 个。 degrees = G.out_degree() top_degree_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:10] print(top_degree_nodes) 计算结果： [(2828, 264), (0, 115), (2700, 86), (2833, 65), (2999, 55), (2560, 53), (2574, 42), (3021, 37), (2651, 36), (2834, 31)] 可以看到下线最多的节点是节点2828有 264 个下线，第二是根节点0有 115 个下线。 Figure 7: 图7 这些节点表现在图中就是像水母一样的中心节点。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:4:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"最大介绍top10 除了通过=度=来衡量一个节点是否为关键节点外，我们还可以通过介数来衡量。 如图 8 所示，根节点 0 传递到节点1,节点2,节点3…. 其中节点2,节点 5 的度非常小，分别为 2 和1,但是如果少了他们的话，后面整个网络就断了。 介数就是表示网络中群体与群体之间的中间人角色，现实生活中如果度数大的是黄牛，那么这个介数的中间人就是给黄牛提供渠道的关键人物。 Figure 8: 图8 我们在图中将前 10 大中介点标记成了绿色，方便查看。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:5:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"完整代码 以上分析的完整代码： import networkx as nx from pyvis.network import Network import pandas as pd # 读取数据创建图 df = pd.read_excel('~/传销原始数据.xlsx') df = df.loc[:, ['推荐人ID', '被推荐人ID']] df.columns = ['source', 'target'] G = nx.from_pandas_edgelist(df, 'source', 'target', create_using=nx.DiGraph()) # 求根节点 top_nodes = [n for n, d in G.in_degree() if d == 0] print('root node:', top_nodes) # 节点层级 level = nx.shortest_path_length(G, 0) nx.set_node_attributes(G, level, 'level') # 显示层级 data = {} for n, l in level.items(): if l in data.keys(): nodes = data[l] nodes.append(n) data[l] = nodes else: data[l] = [n] # 打印前10层节点 for l, n in data.items(): if l \u003c 10: print(l, len(n)) # 打印前10大度节点 degrees = G.out_degree() top_degree_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:10] print(top_degree_nodes) # 给节点添加属性 for node in G.nodes: G.nodes[node]['title'] = str(node) level = G.nodes[node]['level'] # 给节点添加大小属于 G.nodes[node]['value'] = 32 - level # 第一、二、三层节点添加颜色 if level == 0: G.nodes[node]['color'] = 'red' elif level == 1: G.nodes[node]['color'] = 'fuchsia' elif level == 2: G.nodes[node]['color'] = 'purple' # 中介点 center = nx.betweenness_centrality(G) center_tops = sorted(center.items(), key=lambda x: x[1], reverse=True)[:10] # 给前10大中介点添加颜色 for node in center_tops: G.nodes[node[0]]['color'] = 'teal' nx.write_gexf(G, 'test.gexf') nt = Network('650px', '1250px', directed=True) nt.from_nx(G) nt.show('test.html') ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:6:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"结语 networkx是复杂网络分析的利器，搭配上可视化库 pyvis ，可以简单几行代码完成分析和可视化。 ","date":"2022-08-14","objectID":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/:7:0","tags":["网络分析","python","IT审计"],"title":"传销组织层级结构分析","uri":"/posts/20220814001303-%E4%BC%A0%E9%94%80%E7%BB%84%E7%BB%87%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"},{"categories":["工作"],"content":"给一个公众号投了关于 IT 审计的文稿，需要一张作者照片。 但我发现之前没有拍稍微正式点的形象照，所以就一个人跑去照相馆拍了。 说实话，平时我都不拍照，我媳妇用手机给我拍也是抗拒的，每次都是一种不自然的状态。 就听到给我照相的小哥不停的说： “哎～，来微笑” “对，稍微笑一点” “哎～，来～，笑一下” “对，对～，很好，自然一点” “表情僵了，来，自然一点” 我都替小哥着急，想着赶紧拍完了事。 为了后面不再拍照，就一次性选了 9 张照片，花了 900 大洋，有点心疼。 如果有 IT 审计相关的业务咨询可以邮件联系: tujiabing81@163.com ","date":"2022-08-14","objectID":"/posts/20220814214405-%E7%8B%97%E5%93%A5%E5%8E%BB%E7%85%A7%E7%9B%B8%E9%A6%86/:0:0","tags":["杂文"],"title":"狗哥去照相馆","uri":"/posts/20220814214405-%E7%8B%97%E5%93%A5%E5%8E%BB%E7%85%A7%E7%9B%B8%E9%A6%86/"},{"categories":["效率"],"content":"有网友问，怎么批量修改文件最后的修改日期。 比如， excel 文件、 word 文件等。 当然我们先将电脑日期设置成以前的某个日期，然后一个一个打开文件后，修改下，再保存。这样可以完成日期的更换。 今天我们用 python 来实现文件修改日期的批量替换。 比如在=/home/nigo/tmp/test=文件夹下有一些文件，最后修改日期是=2022-07-19=。 我们只需要做两步： 循环获取该文件夹所有文件路径。 修改文件日期 我们先看修改文件日期： import os def change_file_date(path, atime, mtime): \"\"\"改变文件修改日期和访问日期\"\"\" info = os.stat(path) os.utime(path, (atime, mtime)) 我们只需要用os.utime函数就可以将文件的访问日期和修改日期分别改变为atime和 mtime （时间戳数字）。 完整代码如下： import os import datetime def change_file_date(path, atime, mtime): \"\"\"改变文件修改日期和访问日期\"\"\" info = os.stat(path) os.utime(path, (atime, mtime)) def get_file_list(dir, file_list): \"\"\"递归获取文件夹下所有的文件路径\"\"\" newdir = dir if os.path.isfile(dir): file_list.append(dir) elif os.path.isdir(dir): for s in os.listdir(dir): #如果需要忽略某些文件夹，使用以下代码 # if s == \"xxx\": # continue newdir=os.path.join(dir,s) get_file_list(newdir, file_list) return file_list if __name__ == \"__main__\": # 需要修改的文件所在的文件夹 modify_directory = '/home/nigo/tmp/test' # 需要设置成的修改时间：年,月,日,时,分,秒 modify_time = datetime.datetime(2022, 4, 5, 18, 20, 31) # 将时期转化为时间戳 mtime = datetime.datetime.timestamp(modify_time) # 获取指定文件夹下的所有文件路径 paths = get_file_list(modify_directory, []) # 循环所有文件 for path in paths: # 修改文件的访问时间和修改时间 change_file_date(path, mtime, mtime) 我们执行代码将/home/nigo/tmp/test文件夹下的所有文件修改日期改变为2022-04-05 18:20:31。 可以看到所有文件的修改日期已全部批量修改。 当然你会 VBA 的话，也可以使用 VBA 实现，只是用 Python 更快速一点。 ","date":"2022-08-14","objectID":"/posts/20220814214636-%E6%89%B9%E9%87%8F%E6%94%B9%E5%8F%98%E6%96%87%E4%BB%B6%E6%9C%80%E5%90%8E%E4%BF%AE%E6%94%B9%E6%97%B6%E9%97%B4/:0:0","tags":["python"],"title":"批量改变文件最后修改时间","uri":"/posts/20220814214636-%E6%89%B9%E9%87%8F%E6%94%B9%E5%8F%98%E6%96%87%E4%BB%B6%E6%9C%80%E5%90%8E%E4%BF%AE%E6%94%B9%E6%97%B6%E9%97%B4/"},{"categories":["效率"],"content":" 陈版主答疑文章使用的爬虫失效了，暂时没有更新，这周我抽时间更新下，再为大家每天推送。 昨天文章有朋友留言让分享下上交所警示函下载的工具， 虽然写这个代码不难，但还是可以和大家分享下思路。 ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:0:0","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"任务拆解 当我们想做一个工具的时候，首先需要梳理出逻辑。 也就是先手工操作一遍，把一个大任务拆分成可执行的小任务。 ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:1:0","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"大目标 比如，我们的目标是登录上交所网站，输入“警示函”关键字，点击查询， 点击一个列表页，将显示的 PDF 下载下来，然后复制其中的文字到我们保存的文件中。 这个流程有好几步，很多人刚开始学习的时候不太意识到这是一个大目标， 你直接对别人说：“哎，把最近几年上交所警示函内容帮我整理出来。” 别人是茫然的，不知道怎么做。 同样的，你自己也不知道，你可能只能在浏览器上搜索：“批量下载上交所警示函” 如果运气好，别人做过，可能会有现成的轮子，否则，你就又卡住了。 要知道，对于一个大目标我们是很难实际落地执行的。 ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:1:1","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"小任务 那么,要想实现我们的大目标，最好的方法就是任务拆解。 获取网页信息（包括 PDF 下载链接) 下载 PDF 文件 解析 PDF 文件 保存数据 当我们能把任务进行拆解后，难度就自然极度下降了， 我们现在只需要针对这 4 个问题写函数完成。 我们以最简单的解析 PDF 文件为例， 啊？为什么这个是最简单的？因为之前在一篇文章中学过，用 pdfplumber 库解析 PDF 。 import pdfplumber def extract_pdf(path): \"\"\"提取pdf文字内容\"\"\" with pdfplumber.open(path) as pdf: pages = pdf.pages text = ''.join(page.extract_text() for page in pages) return text 你看 4 个问题，我们就解决了一个。简单吧？ ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:1:2","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"周而复始 需要注意的是，上交所给的 PDF 绝大部分是文本的，一小部分的是扫描图片的。 这个代码只能解析文本的 PDF ，如果你想完全解决，那么我们又可以任务拆解的方法， 将3.解析PDF文件分解为： 3.1 解析文字类PDF 3.2 解析扫描类PDF 啊？扫描类的 PDF 文件我怎么解析呢？要么你在浏览器上搜索下有没有这个解决办法。 要么我们再进一步拆解，我们可以把扫描的 PDF 保存为一张张图片,再用 OCR 去识别图片： 3.2.1 PDF拆分成图片 3.2.2 OCR识别图片为文字 每一个如果不会，就去搜索解决，搜索没有直接答案的，就看能不能拆成更小的任务， 循环往复，直到找到解决问题的方法。 ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:1:3","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"搜索问题及笔记记录 通过上面的步骤，我们能把一个复杂任务转换为简单任务。 这也是数学中的化归思想。 这些简单任务，有些我们可能已经会了，有些可能不会。 对于不会的，我们就需要去检索了，也就是问 度娘。 这个真就是熟能生巧了，查得多了，就有技巧了，基本小的问题你都能解决。 当你查到后，一定要把有价值的问题，记录到你的笔记中， 你看上次我们在“ python 提取关键审计事项”的文章中用到的pdfplumber， 今天又用上了。 查一次你可能会忘记，当你记录下来，下次遇到你就节约了检索的时间， 多遇到几次，你就彻底掌握了，这个就是知识习得的过程。 ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:2:0","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"完整代码 其实代码并不重要，有需要的可以自己拿来练习、练习。 import pdfplumber import pandas as pd import requests from urllib.parse import urljoin,urlencode,quote import os import pymysql import time as ttime def extract_pdf(path): \"\"\"提取pdf文字内容\"\"\" with pdfplumber.open(path) as pdf: pages = pdf.pages text = ''.join(page.extract_text() for page in pages) return text def download_pdf(url,path,pdf_name): \"\"\"下载PDF文件\"\"\" file_path = os.path.join(path,pdf_name) if os.path.exists(file_path)==False: if os.path.exists(path)==False: os.makedirs(path) r=requests.get(url) with open(file_path,'wb') as f: f.write(r.content) def get_download_urls(keyword): \"\"\"获取列表信息及PDF下载链接\"\"\" page = 1 url = 'http://query.sse.com.cn/search/getSearchResult.do?search=qwjs\u0026jsonCallBack=\u0026searchword=T_L+CTITLE+T_E+T_L' + quote(keyword) +'+T_R+and+cchannelcode+T_E+T_L0T_D8311T_D8321T_D8348T_D8349T_D8365T_D8703T_D8828T_D8834T_D9856T_D9860T_D9862T_D9888T_D9889T_D9892T_D10004T_D10011T_D10743T_D12002T_D88888888T_RT_R\u0026orderby=-CRELEASETIME\u0026page=%s\u0026perpage=10\u0026_=1660204739688' % page header = { 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36 Edg/103.0.1264.71', 'Host':'query.sse.com.cn', 'Referer':'http://www.sse.com.cn/', } response = requests.get(url,headers=header) json_str = response.json() total_page = int(json_str['countPage']) infos = [] for page in range(total_page+1): ttime.sleep(4) # 暂停的秒数，避免频繁调用 print('获取第%s页信息，共%s页' % (page,total_page)) url = 'http://query.sse.com.cn/search/getSearchResult.do?search=qwjs\u0026jsonCallBack=\u0026searchword=T_L+CTITLE+T_E+T_L' + quote(keyword) +'+T_R+and+cchannelcode+T_E+T_L0T_D8311T_D8321T_D8348T_D8349T_D8365T_D8703T_D8828T_D8834T_D9856T_D9860T_D9862T_D9888T_D9889T_D9892T_D10004T_D10011T_D10743T_D12002T_D88888888T_RT_R\u0026orderby=-CRELEASETIME\u0026page=%s\u0026perpage=10\u0026_=1660204739688' % page response = requests.get(url,headers=header) json_str = response.json() for row in json_str['data']: title = row['CTITLE_TXT'] print(title) link = row['CURL'] url = urljoin(\"http://www.sse.com.cn/\",link) date = row['CRELEASETIME'] time = row['CRELEASETIME2'] file_type = row['MIMETYPE'] id = row['DOCID'] data = { 'title':title, 'url':url, 'date':date, 'time':time, 'file_type':file_type, 'id':id } infos.append(data) return infos def upload_mysql(connect, cursor, item, table_name): \"\"\"上传字典数据到mysql数据库\"\"\" keys = ','.join(item.keys()) values = ','.join(['%s']*len(item)) sql = 'insert into %s(%s) values(%s) on duplicate key update ' % ( table_name, keys, values) update = ','.join([key + '=%s' for key in item]) sql += update try: cursor.execute(sql, tuple(item.values())*2) connect.commit() except: connect.rollback() if __name__ == '__main__': # 使用数据库，如果不将解析的数据传到数据库，可以注释掉,注意修改数据库账号、密码信息 # connect = pymysql.connect( # host = '127.0.0.1', db = 'book', user = 'root', # passwd = 'xxxx', charset = 'utf8') # cursor = connect.cursor() # table_name = 'chufa' # 使用数据库，如果不将解析的数据传到数据库，可以注释掉 directory = './pdf' # 下载的pdf文件保存文件夹路径 infos = get_download_urls('警示函') df = pd.DataFrame(infos) df.to_excel('下载链接.xlsx',index=False) # 将获取到的列表信息保存到本地 output = [] for row in infos: url = row['url'] pdf_name = row['title'] + '.' + row['file_type'] try: download_pdf(url,directory,pdf_name) # 下载PDF print('下载文件：%s' % pdf_name) path = os.path.join(directory,pdf_name) content = extract_pdf(path) except: print('下载文件失败') content = '' row['content'] = content # 传数据库,如果不用数据库可以注释掉 # upload_mysql(connect,cursor,row,table_name) 由于解析的 PDF 文字很多，直接输出成 Excel 会串行，所以我是在第 4 步保存数据的时候， 把数据保存在数据库中，然后把数据库的表导出成 Excel 。 为了让读者能执行代码，我把上传数据库的代码注释了，但是解析的content你就看不到。 如果你会 mysql 数据库，可以把取消注释代码。 如果你想查询其它的关键词，下载 PDF ，可以修改这行代码的关键词： infos = get_download_urls('警示函') ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:3:0","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["效率"],"content":"文件信息及PDF下载 我把下载好的信息和 PDF 也打包分享大家，有需要的可以直接下载： https://pan.baidu.com/s/1tTErvLvPgo0R30WTP9nUvA?pwd=k3sr ","date":"2022-08-14","objectID":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/:4:0","tags":["python"],"title":"上交所警示函批量下载","uri":"/posts/20220814220542-%E4%B8%8A%E4%BA%A4%E6%89%80%E8%AD%A6%E7%A4%BA%E5%87%BD%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"},{"categories":["工作"],"content":"最近在投一个资金分析的标，需要对几百个账户资金数据进行分析。 一般我们审计一家企业的时候，是很难对资金数据进行穿透的，为什么？ 因为我们只有被审计企业的资金账户数据，他的上游、下游的数据我们都不可能获取到。 所以，我们无法对整个资金链进行穿透。 也就是说，最困难的点是我们没有*数据*。 而这个项目最有意思是公安机关能获取到所有数据，我们需要做的就是分析。 那么在有数据的情况下怎么去分析呢？ 用 Excel 肯定不行吧，几百个账户，上亿的流水，眼睛去看或者用公式去看都很难。你很难直观知道这个钱分批转出去后，最终到了哪些账户里。 这就需要复杂网络分析的技术，去找网络中资金走的什么链路，最终归集到哪里，哪些节点是关键节点等等？ 所以这几天我正在学习复杂网络分析的相关知识，以及 python 的=networkx=库。 我非常期待能做这个项目，并把学到的知识运用到实战中。 ","date":"2022-08-14","objectID":"/posts/20220814214247-%E5%AD%A6%E4%B9%A0/:0:0","tags":["IT审计"],"title":"学习","uri":"/posts/20220814214247-%E5%AD%A6%E4%B9%A0/"},{"categories":["工作"],"content":"需求为向导 我一般不太喜欢去为了学习一个新的知识而学习新的知识，或者是为了考证而考证。 我喜欢在工作中看什么东西对我有用，我再去针对性地去学习，也就是以需求为向导地学习。 这样会有非常大的动力，并且学习过程中，能立马解决现实中的问题，带来强烈的正向反馈，进而促使我投入更多精力去掌握这些新的知识。 随着用这些知识解决问题数量的增多，就会慢慢从入门到精通。 而这个过程是非常有趣的，娱乐的，我感觉很多时候比玩游戏、刷剧有趣多了。 我基本休息的时间，都在干类似的事情，乐于其中。 祝读者周末愉快！ ","date":"2022-08-14","objectID":"/posts/20220814214247-%E5%AD%A6%E4%B9%A0/:1:0","tags":["IT审计"],"title":"学习","uri":"/posts/20220814214247-%E5%AD%A6%E4%B9%A0/"},{"categories":["效率"],"content":"前面在所里的时候，一位经理问我一些效率工具使用的问题。 我把以前做的东西告诉了她大概怎么使用的，然后给她了一个以前整理过的链接，让她自己去看。 聊到最后，我就说搞这些就是浪费自己时间，方便别人，而且还不能升职。 她也点点头，认同这个事实。 当时初、中级的时候就学了很多这些，也做了很多工具，当然完全是为了方便我自己不去做重复的工作，这是一个理念的问题。 以前那种 VBA 工具，我现在一个都没有去做了。不是因为我理念变了，或者说正常工作多了，而是我完全没有这些需求，我做它干嘛？ 我折腾的都是自己工作上用得上的东西，能帮助我不做重复工作的事。 只是每个时间段，学的东西不一样。 我仍然还是每天浪费了很多时间在学东西上面。 最近我从 vim 编辑器转到了 emacs 编辑器上面， 一个是编辑器之神，一个是神之编辑器。很多时候就是纯粹浪费时间。 但这种浪费时间，对于我来说是娱乐的，类似于喜欢打游戏的人的一样，一种消遣方式。 比如，上周有同事，让我把上交所的警示函给她找出来。 网站上是 pdf 文件。 由于我以前浪费过时间折腾过 python ，所以也就帮忙写了个爬虫批量把 PDF 下载下来，然后读取 PDF 的内容，整理成 Excel 。 最后发给她所有信息，以及打包的 PDF 文件。 不过我好像自己又浪费了几个小时时间。 ","date":"2022-08-13","objectID":"/posts/20220813233123-%E6%88%91%E6%B5%AA%E8%B4%B9%E4%BA%86%E5%BE%88%E5%A4%9A%E6%97%B6%E9%97%B4/:0:0","tags":["杂文"],"title":"我浪费了很多时间","uri":"/posts/20220813233123-%E6%88%91%E6%B5%AA%E8%B4%B9%E4%BA%86%E5%BE%88%E5%A4%9A%E6%97%B6%E9%97%B4/"}]