+++
title = "AI在消费级电脑上改变我们的学习"
date = 2024-02-25
lastmod = 2024-04-13T23:25:51+08:00
tags = ["AI"]
categories = ["工作"]
draft = false
author = "nigo"
+++

自2023年12月以来，AI已经经历了爆发式发展，已经深刻改变了很多工作场景。

它就像一个杠杆，可以将人的能力放大，一个人可以完成曾经一个团队完成的事。

由于人接受新事物能力的不同，对一些IT技术掌握的差距，导致实际上还是有很多人并没有深入使用过AI。

几个月前，除了以 chatgpt ，Midjourney 为代表的 AI 应用，可供大家消遣外，要想在本地使用AI，还需要一张好的显卡，以及一些IT技术门槛。

然而，这两天我使用了 `ollama` 来本地部署各种大模型，我发现只需要你有 4、5 G 显存的显卡，一点简单操作就可以在本地玩转大模型了。

也就是说 AI 已经可以在我们消费级电脑上方便使用了。

有了本地模型，对于我们普通人来说，可以干很多事情了，至少可以改变我们学习了。

( 注：以下示例，只需要连接本地部署的 ollama 即可实现 )


## 给笔记系统插上 AI 翅膀 {#给笔记系统插上-ai-翅膀}

作为知识的沉淀，我有自己的笔记系统，这是我所有工作、学习、写作的记录。

我相信很多少都会有使用类似的软件，如 obsidian 、logseq 、notion 、vimwiki 、org-roam 。

这些都是非常适合作为第二大脑的笔记系统，而现在我们就可以将它插上 AI 的翅膀。

比如，我在写篇文章时，想引用下关于=人工智能=的名言，显得我比较有文采。

那么我只需要在我常用的 emacs 编辑器 上按几个快捷键就可以得到我要的答案：

( 注：我使用的 emacs 编辑器的 org-roam 插件作为我的双链笔记系统。使用 ellama 插件连接部署的 ollama)

{{< figure src="/ox-hugo/2024-02-25_21-34-37_ellama-名言.gif" >}}

对于笔记我想总结下全文内容，也只需要按下快捷键就可以完成：

{{< figure src="/ox-hugo/2024-02-25_21-48-34_ellama-总结.gif" >}}

像平时的IT审计项目中，会用 emacs 写些 python ，简单的也可以不换到 chatgpt 上，直接在编辑器里完成了：

{{< figure src="/ox-hugo/2024-02-25_21-56-31_ellama-代码.gif" >}}

当然，这只是举了一些例子。借助 ollama 我在笔记系统中可以干很多事情：

-   翻译
-   名词解释
-   总结笔记
-   总结网页内容
-   文章润色
-   代码编写
-   代码审阅
-   代码修改

等等。

最关键的是这是集成在我的笔记系统的，也就是不再需要通过在 chatgpt 上问一下再整理到我的笔记中了。


## 给 Marginnote 插上 AI 翅膀 {#给-marginnote-插上-ai-翅膀}

在我们学习、考试过程中，可能需要使用到 PDF 阅读器、思维导图工具、复习工具等。

而 Marginnote 是把这些工具的集成者（“我要打10个”）。

而使用=MN ChatAI=插件，我们可以连接 chatgpt 或者 chatglm 等接口，可以让我们学习过程中使用上 AI 。

( 注：该插件本身提供内置每天 100 次查询额度，不够的也可以注册清华智谱的 chatglm 接口。我使用自己部署 ollama 的 chatgpt 格式的 api ，可以无限次调用 )

在我学习过程中，经常遇到一些要记忆的内容，很多时候我们可以使用一些记忆法助记，比如很多人喜欢搞些“xxx背诵卷”，别人给你用谐音法之类总结好了，确实很香，对吧？

{{< figure src="/ox-hugo/2024-02-25_22-37-04_screenshot.png" >}}

但也不是所有材料都有这种现成东西，而利用 AI 我们可以很容易辅助我们去记忆。

下面是简短的操作演示：

当然，在我们学习的过程中，经常会出现一些专业名词，虽然在 marginnote 中可以使用“研究”来上网查询，但利用 AI 明显是更快，结果也更干净。

下面是简短的操作演示：

如果你觉得这不够惊艳，那么你实际上可以根据你学习的需要自己去创造。

比如，前面我让 AI 帮助我记忆，只是告诉他作为我的记忆助手，大概需要做什么事。你完全可以自定义一些 prompt 模板，完成你学习需要的任务。

{{< figure src="/ox-hugo/2024-02-25_22-44-05_screenshot.png" >}}

这才是它连接 AI 最强大的地方。


## 参考文章及推荐 {#参考文章及推荐}

本文是利用 ollama 安装了一些大模型，在个人笔记和学习过程中使用的一些尝试。

推荐大家自己去试试 ollama ，就算你没有显卡，也可以去试试，他可以使用 cpu 运算（会非常慢).

( 支持 linux ，Mac ，Windows ）

参考文章：

ollama 官网：<https://ollama.com/>

用 Ollama 轻松玩转本地大模型：<https://sspai.com/post/85193>

ollama github: <https://github.com/ollama/ollama>

安装好 ollama 后，就可以一键装一些开源大模型。

根据我的试用情况推荐以下支持中文较好的模型，排名按我觉得好用的顺序：

-   Yi: 我使用的 yi:6b-chat-q8_0 占用显存或者内存 6.4 G.
-   qwen: 我使用的 qwen:14b 占用显存或者内存 8.2 G.
-   llama2-chinese: 我使用的13b 占用显存或者内存 7.4 G.

以上模型，都有更高参数或更低参数版本，由于我显卡显存是 10 G,我选择的都是更接近我显存的。

实际上，只要两三G 的显存或内存，你就可以玩起来。

至于你想找什么模型，你可以在官网（<https://ollama.com/>) 上搜索你要的模型。

{{< figure src="/ox-hugo/2024-02-25_22-56-35_screenshot.png" >}}

搜索进去后，点击“Tags" 你可以看到不同参数大小的模型，右边就是运行安装的命令，一键完成。

你可以清晰看到模型的大小，可以选择和自己显存大小相近的，非常方便。

{{< figure src="/ox-hugo/2024-02-25_22-57-08_screenshot.png" >}}

以我 Linux 系统为例，只需要按照官网方法两步完成：

1.  安装 ollama:

在终端中输入命令：

{{< highlight bash >}}
curl -fsSL https://ollama.com/install.sh | sh
{{< /highlight >}}

{{< figure src="/ox-hugo/2024-02-25_23-00-07_screenshot.png" >}}

1.  下载模型

根据你想要的模型名称不同 `pull` 对应的名称。（可以参考前面我推荐的中文模型）

{{< highlight bash >}}
ollama pull llama2-chinese
{{< /highlight >}}

这就完成了！非常简单。

1.  使用

在终端上我们输入想问的问题：

{{< highlight bash >}}
ollama run llama2-chinese "天空为什么是蓝色的？"
{{< /highlight >}}

就可以得到 AI 的回答。

当然，文章前面我示例的应用，都是一些软件对应的 ollama 连接插件。

目前支持的笔记软件有：

-   emacs
-   neovim
-   obsidian
-   logseq

由于 ollama 支持提供 chatgpt 格式的 api 所以，其实基本上很多支持 AI 插件的软件都可以与其连接。

在不久的将来，随着消费级电脑的显卡算力的增强，我们将可以在本地跑更大参数的模型，

借助 AI 的力量，我们可以更好的学习、工作。
