+++
title = "metabase数据分析及可视化-先导篇"
date = 2023-08-13T18:29:00+08:00
lastmod = 2023-08-13T18:29:54+08:00
tags = ["metabase"]
categories = ["工作"]
draft = false
author = "nigo"
+++

我们将数据分析过程可以简单划分为：

1.  数据清洗。
2.  数据分析。
3.  数据可视化。

其实这三部分都非常重要，缺一不可，任何一个环节没有做好，都可能导致我们数据分析工作的延误或失败。

根据我做过一些数据核查项目的经验，我对上述三个步骤的感性认识进行介绍。


## 数据清洗 {#数据清洗}

如果你是在企业做数据分析，这些数据可能已经在数据库的，并不需要数据清洗的这个步骤，可以直接上手分析。

但像我们作为中介机构对企业进行数据核查，那么第一件事就是数据清洗。

{{< figure src="/ox-hugo/2023-08-13_16-19-16_screenshot.png" >}}

在我看来，一般当出现这两种情况的时候会花费较多时间：


### 多文件手工数据 {#多文件手工数据}

如果给到我们的数据是手工维护的，再加上按月、渠道等又分成多个文件。

那么将是比较麻烦的事。

当然，如果是格式一样，我们可以通过一些Excel VBA插件或者 python 能够批量处理。

但是，手工维护的往往不可能那么规范，因此前期清洗、合并数据将会费不少工夫。


### 大数据量文件 {#大数据量文件}

其实当数据量较少的时候（比如：几百兆），无论是 Excel 还是 CSV 文件大家用软件都还能打开。

所以，要去除其中特殊符号，不规范的字符等都还可以操作。

但只要数据量一大，大家可能就会碰到打不开，看不见，导不进的困境。

死死地卡在前期的数据清洗工作中。

其实这种情况下是有技巧和方法的，我们可以借助终端工具快速完成合并文件、替换文本、删除特殊字体、掐头去尾等操作的。

这些技巧，一般教程讲得比较少，主要靠自己学习总结。


## 数据分析 {#数据分析}

数据清洗完成后，我们利用 SQL 或者 Python 对数据进行查询分析，之前我们介绍过的 mysql 、duckdb 、clickhouse 、python 基本都是在完成这个目标。

{{< figure src="/ox-hugo/2023-08-13_16-34-38_screenshot.png" >}}

其实，这部分工作相对来说，简单易学一些，对于常用的分析维度、分析指标、分析方法，写几次 SQL 就会了。

难度不大。


## 数据可视化 {#数据可视化}

数据分析我们往往形成的是报表，有时候并不能直观发现问题及异常点，因此我们需要对数据进行可视化，当然这也是汇报展示时需要用到的。

{{< figure src="/ox-hugo/2023-08-13_16-38-36_screenshot.png" >}}

这类工具有非常多

1.  Excel, Power BI.
2.  商业软件，ACL,Tableau 等。
3.  Python 绘图库，Plotly,Seabon等。
4.  开源 BI 工具，Superset,MetaBase,ReDash 等。

我花了很长时间，使用 Python 的 Plotly 库作为我的可视化工具，因为它能贯穿我的整个工作流，并且也具有快速和出图美观的特点。

但这肯定不是适合所有人的，毕竟它需要比较大的学习成本和时间成本。

那么我还是推荐大家使用开源的 BI 工具。之前我使用过基于 Python 的 Superset ，当时试用过下，最终还是没有继续使用下去。

前两天和所里创新研发部沟通的时候，了解到 MetaBase 这个工具，我试用了下，无论从易用性、美观度、功能都很不错。

因此，我准备自己学习使用下，同时写个系列教程文章进行介绍。


## MetaBase {#metabase}

MetaBase 是一款开源免费的 BI 系统。它面向的是非技术人员，因此使用起来比较易用，通过拖拖拽拽就可以形成比较美观的图表。

{{< figure src="/ox-hugo/2023-08-13_17-01-28_screenshot.png" >}}

同时对于技术人员，也可以直接写 SQL ，将 SQL 查询结果出图表。

{{< figure src="/ox-hugo/2023-08-13_17-05-00_screenshot.png" >}}

通过它我们可以连接多种常用的数据库类型，这样我们直接可以和我们第二步数据分析的环节打通，直接在这上面写 SQL 出图。

{{< figure src="/ox-hugo/2023-08-13_17-14-38_screenshot.png" >}}

{{< figure src="/ox-hugo/2023-08-13_17-15-04_screenshot.png" >}}

同时它还有比较强大的透视和下钻功能：


### 透视 {#透视}

当我们只要连接了某个数据，就可以直接对其中的数据表进行`透视`。

他会自动将一些基本的分析维度的可视化结果展示给我们：

{{< figure src="/ox-hugo/2023-08-13_17-26-28_screenshot.png" >}}

当然，在“更多透视方法”中也给我们提供了更多的字段，当我们关心某个字段时，可以点击进去。

比如我对created_at创建时间更感兴趣，我直接点击，就直接显示出时间序列的分析结果：

按年分析：

{{< figure src="/ox-hugo/2023-08-13_17-30-19_screenshot.png" >}}

{{< figure src="/ox-hugo/2023-08-13_17-31-34_screenshot.png" >}}

按月、按季度、按星期、按小时分析：

{{< figure src="/ox-hugo/2023-08-13_17-32-20_screenshot.png" >}}

这些工作，我们还没有写一句 SQL ，全是它自动完成的。对于我们快速了解数据整体非常有帮助。


### 下钻 {#下钻}

除此之外，我们还可以对我们关心的数据直接进行下钻和横向对比。

例如，下图中我看到美国的数值很大，我想看下是什么原因导致的，或者是其具体情况，我可以直接点击“透视”

{{< figure src="/ox-hugo/2023-08-13_17-34-32_screenshot.png" >}}

我们可以直接看到地区是美国的详细情况。

{{< figure src="/ox-hugo/2023-08-13_17-36-29_screenshot.png" >}}

甚至我们还可以点击“与其他国家对比”，直接看美国和所有国家的对比情况。

{{< figure src="/ox-hugo/2023-08-13_17-38-59_screenshot.png" >}}

这一点其实非常有用的，因为我们经常在分析的过程的会看到异常波动的点，我们需要对其按各个维度对分析主要是什么原因导致的。

它的这个透视、下钻、横向对比功能，将大大减轻我们的工作。


## 结语 {#结语}

我们可以看到 MetaBase 是非常强大和方便的，但是有一个问题是我们需要对表的字段设置成正确的数据类型。

比如，一般我们在 Mysql 导入数据时可能为了图方便，直接全部字段设置成varchar(255)的文本类型。

但在 MetaBase 中进行可视化时，这种文本类型无法直接作为数值进行计算出图。

同样的，日期时间也需要正确设置成 DATETIME 、DATE 等数据类型，才可以进行时间序列分析。

这其实对我们数据核查来说提出了比较高的要求，因为我们面临大量的数据导入工作。

这将给我们增加大量建表工作。

因此，在我们正式安装学习之前，下一篇我会先介绍如何一键快速建表和导数。

这个过程会比使用图形化工具的导入向导更快。

那我们就先安利到这吧，下篇见。
